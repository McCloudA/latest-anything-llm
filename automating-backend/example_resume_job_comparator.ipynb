{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example - job embedding, resume embedding, job match generator\n",
    "\n",
    "Here's what we're doing here:\n",
    " - Pulling in Job Descriptions (from a file here, this should be with a daily DB Pull) and using an LLM to structure them into several relevant text sections\n",
    " - Pulling in a desired Resume (again, from a file here, this should come from S3 on demand) and getting it into text format (from .PDF or .DOCX format)\n",
    " - Using an LLM to structure the resumes into several relvant text sections\n",
    " - Using python libraries we wrote to provide embedding for both resumes and job descriptions (here stored locally, these should go into our DB to be used later)\n",
    " - Using similarity metrics to generate best jobs for a person (to be done on demand or daily)\n",
    "\n",
    "Then we have a few examples to look at.  This code is designed to be an example of how to use all these libraries."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tunable parameters and inputs\n",
    "Here we get to decide:\n",
    " - Where the scraped jobs file is for today, and our credential file\n",
    " - The \"truncation list\" of vector embeddings (for example, [20,75] would mean \"first compare the first 20 dimensions of an embedding vector for similarity, then do a 75-length vector of the ones that pass, then finally do all dimensions)\n",
    " - relevant fields to be compared for both jobs and applicants\n",
    " - A corresponding threshold list (cosine similarity threshold that each comparison must pass)\n",
    "\n",
    " ## NOTE:\n",
    "\n",
    "At present, the files below have to exist.  Soon we will be reading from an S3 location instead of a static json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# TODO:  Put the BOW/OpenAI embedders on a switch here\n",
    "# TODO:  Something needs fixing - now we have a threshold on the compairison between 'jobsWanted_concat' and jobs \"title,\"\n",
    "#        yet we're recommending jobs that have nothing in common in those fields, so should have a dot product of 0.  Figure it out!\n",
    "# where today's scraped jobs are:\n",
    "use_local_scraped_file = False # if true, use the file below.  If false, use the S3 bucket below that.\n",
    "scraped_jobs_json_filename = './example_data_fromdb/new-Patagonia-jobs_05-28-2023.json'\n",
    "\n",
    "s3_scraped_bucket_name = 'jobs-scraped-uploads'\n",
    "s3_scraped_bucket_prefix = ''\n",
    "datestring = str(datetime.today().strftime('%m-%d-%Y'))\n",
    "s3_scraped_bucket_filename = '_final_' + datestring  + '.json'\n",
    "\n",
    "#and here's where we'll put the file (if on S3)\n",
    "s3_scraped_bucket_outfilename = '_final_recommendations_' + datestring  + '.json'\n",
    "\n",
    "\n",
    "# our credentials file:\n",
    "credentials_filename = \"./credentials.yml\"\n",
    "\n",
    "# for using the openai embedder:\n",
    "#vector_trunc_list = [20] \n",
    "# for using the Bag of Words embedder\n",
    "vector_trunc_list = [] \n",
    "\n",
    "vector_comparison_threshold_list = [0.4]\n",
    "full_vector_comparison_threshold = 0.05 # 0.4\n",
    "num_top_jobs = 3\n",
    "\n",
    "#note that the first comparison field carries the \"full weight\" final comparator.\n",
    "#jobs_relevant_fields = ['fullJobDescription','title']\n",
    "#applicant_relevant_fields = ['fullText','title']\n",
    "#applicant_relevant_fields = ['resume_rawtext']\n",
    "#jobs_relevant_fields = ['fullJobDescription']\n",
    "\n",
    "#applicant_relevant_fields = ['resume_rawtext','jobsWanted_concat']\n",
    "#jobs_relevant_fields = ['fullJobDescription','fullJobDescription']\n",
    "\n",
    "#applicant_relevant_fields = ['resume_rawtext','jobsWanted_concat']\n",
    "#jobs_relevant_fields = ['fullJobDescription','title']\n",
    "\n",
    "#applicant_relevant_fields = ['locations_concat', 'jobsWanted_concat','resume_rawtext']\n",
    "#jobs_relevant_fields = ['locations','title','fullJobDescription']\n",
    "\n",
    "#applicant_relevant_fields = ['resume_rawtext','locations_concat', 'jobsWanted_concat']\n",
    "#jobs_relevant_fields = ['fullJobDescription','locations','title']\n",
    "\n",
    "#last known good\n",
    "#applicant_relevant_fields = ['resume_rawtext', 'jobsWanted_concat']\n",
    "#jobs_relevant_fields = ['fullJobDescription','title']\n",
    "\n",
    "# experimental\n",
    "applicant_relevant_fields = ['locations_concat','jobsWanted_concat','resume_rawtext']\n",
    "jobs_relevant_fields = ['locations_concat','title','fullJobDescription']\n",
    "\n",
    "#applicant_relevant_fields = ['jobsWanted_concat']\n",
    "#jobs_relevant_fields = ['title']\n",
    "\n",
    "uid_name_jobs = 'uuid'\n",
    "uid_name_applicant = 'customer_uuid'\n",
    "\n",
    "# Specify the name of the DynamoDB table and s3 buckets wwhere the resumes can be found:\n",
    "dynamo_people_table_name = 'cheeki-job-automation-user-table'\n",
    "dynamo_appliedjobs_table_name = 'cheeki-jobs-applied-to'\n",
    "\n",
    "s3_bucket_name = 'job-automation-uploads'\n",
    "s3_bucket_prefix = 'public'\n",
    "\n",
    "# desired_job_file = './example_data_fromdb/example_resume_files/Vickers Financial_Ryan Tang_Compliance Officer.pdf'\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API Credentials and Includes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "#!pip install sklearn\n",
    "#!pip install exceptions\n",
    "#!pip install boto3\n",
    "#!nltk.download('stopwords')\n",
    "from jd_tools import ContentEmbedder, JobScorer, CheekiFileHandler\n",
    "\n",
    "\n",
    "import yaml\n",
    "#!pip install boto3\n",
    "#!pip install python-docx\n",
    "import boto3\n",
    "import numpy as np\n",
    "import json\n",
    "#!pip install nltk\n",
    "\n",
    "\n",
    "with open(credentials_filename, 'r') as ymlfile:\n",
    "   cfg = yaml.safe_load(ymlfile)\n",
    "   our_chatgpt_key = cfg['creds']['chatgpt_key']\n",
    "   # AWS credentials\n",
    "   aws_access_key_id = cfg['creds']['aws_access_key']\n",
    "   aws_secret_access_key = cfg['creds']['aws_secret_key']\n",
    "   aws_region_name = 'us-west-2'  # Replace with your desired AWS region\n",
    "\n",
    "\n",
    "# set up the variou handlers we'll need:\n",
    "filehandler = CheekiFileHandler()\n",
    "\n",
    "#Set up our job embedder:\n",
    "job_embedder = ContentEmbedder()\n",
    "job_embedder.add_api_key(our_chatgpt_key)\n",
    "\n",
    "#Set up our applicant embedder:\n",
    "applicant_embedder = ContentEmbedder()\n",
    "applicant_embedder.add_api_key(our_chatgpt_key)\n",
    "\n",
    "\n",
    "# Connect to DynamoDB\n",
    "dynamodb = boto3.resource('dynamodb', aws_access_key_id=aws_access_key_id,\n",
    "                          aws_secret_access_key=aws_secret_access_key,\n",
    "                          region_name=aws_region_name)\n",
    "# and to S3 for file IO\n",
    "s3 = boto3.client('s3', aws_access_key_id=aws_access_key_id, \n",
    "                  aws_secret_access_key=aws_secret_access_key, \n",
    "                  region_name=aws_region_name)\n",
    "\n",
    "\n",
    "# TODO: Use the new S3 URL to get resumes instead of searching for them(?).\n",
    "# TODO: deal with openai resume length truncation, consider pre-processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get and Encode the Job Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_local_scraped_file:\n",
    "    with open(scraped_jobs_json_filename) as f:\n",
    "        jobs_data_raw = json.load(f)\n",
    "else:\n",
    "    s3.download_file(s3_scraped_bucket_name, s3_scraped_bucket_filename, './temp_files/' + s3_scraped_bucket_filename)\n",
    "    #content = filehandler.text_from_file('./temp_files/' + s3_scraped_bucket_filename)\n",
    "    #print(content)\n",
    "    #jobs_data_raw = json.load('./temp_files/' + s3_scraped_bucket_filename)\n",
    "    with open('./temp_files/' + s3_scraped_bucket_filename) as f:\n",
    "        jobs_data_raw = json.load(f)\n",
    "\n",
    "print(jobs_data_raw[0:1])\n",
    "for row_ind in range(len(jobs_data_raw)):\n",
    "    jobs_data_raw[row_ind]['locations_concat'] = ' '.join(jobs_data_raw[row_ind]['locations'])\n",
    "\n",
    "\n",
    "#make locations_concat for us later on (from 'locations):\n",
    "\n",
    "#and set up our BOW embedder using all the available job text:\n",
    "# job_embedder.setup_bow_embedder([i['fullJobDescription'] for i in jobs_data_raw])\n",
    "job_embedder.setup_bow_embedder([i['fullJobDescription'] for i in jobs_data_raw] + [i['locations_concat'] for i in jobs_data_raw])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jobs_data_raw2 = jobs_data_raw.copy()\n",
    "# for row_ind in range(len(jobs_data_raw2)):\n",
    "#     jobs_data_raw2[row_ind]['locations_concat'] = ' '.join(jobs_data_raw2[row_ind]['locations'])\n",
    "\n",
    "# jobs_data_raw2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_embedded_json_list = job_embedder.embed_content(jobs_data_raw,jobs_relevant_fields, vector_trunc_list,uid_name_jobs)\n",
    "#for debug:\n",
    "#jobs_embedded_json_list[0:1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get and Encode Applicant's Resumes\n",
    "\n",
    "Remember: in our product, we want to basically do this on-demand as people put their info into the system.\n",
    "\n",
    "Here we're going to: \n",
    " - Get a pdf or .doc file\n",
    " - grind it up to get the text out of it\n",
    " - (optionally - TODO) use an LLM to subdivide the resume into text segemnts\n",
    " - embed it in the way that we did the jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Connect to DynamoDB\n",
    "# dynamodb = boto3.resource('dynamodb', aws_access_key_id=aws_access_key_id,\n",
    "#                           aws_secret_access_key=aws_secret_access_key,\n",
    "#                           region_name=aws_region_name)\n",
    "# # and to S3 for file IO\n",
    "# s3 = boto3.client('s3', aws_access_key_id=aws_access_key_id, \n",
    "#                   aws_secret_access_key=aws_secret_access_key, \n",
    "#                   region_name=aws_region_name)\n",
    "\n",
    "# Retrieve all applicant records from the DynamoDB table\n",
    "table = dynamodb.Table(dynamo_people_table_name)\n",
    "table_response = table.scan()\n",
    "\n",
    "people_uids = []\n",
    "resume_fulltexts = []\n",
    "applicant_records = []\n",
    "\n",
    "\n",
    "# Extract the 'FileName' attribute from each record\n",
    "counter = 0\n",
    "for item in table_response['Items']:\n",
    "    #print(item)\n",
    "    file_name = item.get('resumeFileName')\n",
    "    is_complete = item.get('isInitialProfileFormCompleted')\n",
    "\n",
    "    id = item.get('id')\n",
    "    if file_name and is_complete:\n",
    "        #print(id,file_name)\n",
    "        #get all the files in the folder\n",
    "        s3_response = s3.list_objects_v2(Bucket=s3_bucket_name, Prefix=s3_bucket_prefix + '/' + id)\n",
    "        # pull the first one that matches our filename\n",
    "        #s3_folder_URI = s3_prefix_resumes + id\n",
    "        target_file = None\n",
    "        #print(s3_response)\n",
    "        for obj in s3_response['Contents']:\n",
    "            if file_name in obj['Key']:\n",
    "                target_file = obj['Key']\n",
    "                break\n",
    "        if target_file is not None:\n",
    "            #print(s3_bucket_name)\n",
    "            if counter % 10 == 0:\n",
    "                print(target_file)\n",
    "            counter = counter + 1\n",
    "            #print('./temp_files/' + target_file)\n",
    "            s3.download_file(s3_bucket_name, target_file, './temp_files/' + file_name)\n",
    "            content = filehandler.text_from_file('./temp_files/' + file_name)\n",
    "            #print(content)\n",
    "            if len(content) > 1:\n",
    "                #we have a resume file, and one we managed to read.  Hang onto these:\n",
    "                people_uids.append(id)\n",
    "                resume_fulltexts.append(content)\n",
    "                locs_wanted = item.get('locations')\n",
    "                if locs_wanted is None:\n",
    "                    locs_wanted = ' '\n",
    "                else:\n",
    "                    locs_wanted = ' '.join(locs_wanted)\n",
    "                #print(item.get('locations'))\n",
    "                applicant_records.append({\n",
    "                    'customer_uuid': id,\n",
    "                    'resume_rawtext':content,\n",
    "                    'payRequest':item.get('desiredSalaryRange'),\n",
    "                    'locations':item.get('locations'),\n",
    "                    'membershipLevel':item.get('membershipLevel'),\n",
    "                    'jobsWanted_concat':' '.join(item.get('jobsWanted')),\n",
    "                    'locations_concat':locs_wanted\n",
    "                })\n",
    "                # print(item)\n",
    "            \n",
    "        #s3://job-automation-uploads/public/591c63c0-3933-4a74-87a8-b460dccb6126/resume--1682308070874-Robert_Shaw-bk_sans_009106 (3).pdf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And make all the structured resume/data sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#people_uids\n",
    "#resume_fulltexts\n",
    "applicant_records[0:2]\n",
    "#jobs_embedded_json_list\n",
    "\n",
    "applicant_records[0]['resume_rawtext']\n",
    "\n",
    "#test_embedder = LLMEmbedder()\n",
    "#test_embedder."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(the version that uses a local pdf file instead - deprecated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf_fulltext = pdfminer.high_level.extract_text(desired_job_file)\n",
    "\n",
    "# #structure this text into our desired format\n",
    "# # NOTE: Replace this with the applicant UUID, and a database query to get their record:\n",
    "\n",
    "# applicant_record = {'title': 'Senior Accommodation Specialist',\n",
    "#   'payRequest': {'lowestPay': 103450,\n",
    "#   'targetHiringPay': 130000},\n",
    "#   'yearsExpParsed': {'Years': 7},\n",
    "#   'workType': [],\n",
    "#   'fullText': pdf_fulltext,\n",
    "#   'customer_uuid': 'e5d175a0-88eb-45e7-acc2-27f0ae5e4257'}\n",
    "\n",
    "#[len(i[applicant_relevant_fields[0]]) for i in applicant_records]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applicant_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#debug - just do a few people\n",
    "#applicant_embedded_json_list = [job_embedder.embed_content([i],applicant_relevant_fields, vector_trunc_list,uid_name_applicant) for i in applicant_records[0:3]]\n",
    "\n",
    "#score all the people:\n",
    "#applicant_embedded_json_list = [job_embedder.embed_content([i],applicant_relevant_fields, vector_trunc_list,uid_name_applicant) for i in applicant_records]\n",
    "\n",
    "applicant_embedded_json_list = []\n",
    "for i in range(len(applicant_records)):\n",
    "    len_str = len(applicant_records[i][applicant_relevant_fields[0]])\n",
    "    print('trying a string with ' , len_str, 'elements:')\n",
    "    job_embedded_temp = job_embedder.embed_content([applicant_records[i]],applicant_relevant_fields, vector_trunc_list,uid_name_applicant)\n",
    "    applicant_embedded_json_list.append(job_embedded_temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applicant_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applicant_embedded_json_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAVE POINT - just to save compute/API costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#dumb global pickle handlers\n",
    "def psave(filename, *args):\n",
    "    # Get global dictionary\n",
    "    glob = globals()\n",
    "    d = {}\n",
    "    for v in args:\n",
    "        # Copy over desired values\n",
    "        d[v] = glob[v]\n",
    "    with open(filename, 'wb') as f:\n",
    "        # Put them in the file \n",
    "        pickle.dump(d, f)\n",
    "\n",
    "def pload(filename):\n",
    "    # Get global dictionary\n",
    "    glob = globals()\n",
    "    with open(filename, 'rb') as f:\n",
    "        for k, v in pickle.load(f).items():\n",
    "            # Set each global variable to the value from the file\n",
    "            glob[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose, but choose wisely.\n",
    "\n",
    "psave('./cheeki_jobfinderdigest_bow.pkl','applicant_records','jobs_data_raw','jobs_embedded_json_list', 'applicant_embedded_json_list', 'jobs_relevant_fields', 'applicant_relevant_fields', 'vector_trunc_list','uid_name_jobs', 'uid_name_applicant','vector_comparison_threshold_list','full_vector_comparison_threshold')\n",
    "\n",
    "pload('./cheeki_jobfinderdigest_bow.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isinstance(np.zeros((10,10)),np.ndarray)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time to find the best jobs for our applicants\n",
    "\n",
    "So here's a library that takes in job data and an applicant's data, and compares the relevant data to find the best matches!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decimal import Decimal\n",
    "\n",
    "class DecimalEncoder(json.JSONEncoder):\n",
    "  def default(self, obj):\n",
    "    if isinstance(obj, Decimal):\n",
    "      return str(obj)\n",
    "    if isinstance(obj,np.ndarray):\n",
    "      return obj.tolist()\n",
    "    return json.JSONEncoder.default(self, obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "applicant_embedder = JobScorer()\n",
    "job_recommendataions_final = []\n",
    "\n",
    "#prevjobs_table = dynamodb.scan(TableName=dynamo_appliedjobs_table_name,FilterExpression=f\"userId = '{userId_value}'\",ProjectionExpression='id')\n",
    "#previously_applied_uuids = [item['id']['S'] for item in response['Items']]\n",
    "\n",
    "table = dynamodb.Table(dynamo_appliedjobs_table_name)\n",
    "\n",
    "\n",
    "print(str(len(applicant_embedded_json_list)), ' Total Applicants...')\n",
    "for applicant_ind in range(len(applicant_embedded_json_list)):\n",
    "    #for applicant_ind in range(25):\n",
    "    print('looking at applicant #' , str(int(applicant_ind)))\n",
    "    print('Jobs wanted: ', applicant_records[applicant_ind]['jobsWanted_concat'])\n",
    "    # find which jobs this applicant has already been applled to:\n",
    "    #print( applicant_embedded_json_list[applicant_ind])\n",
    "    applicant_uuid = applicant_embedded_json_list[applicant_ind][0]['customer_uuid']\n",
    "    #print(applicant_uuid)\n",
    "    #fe = \"userId <> :\" + applicant_uuid + \"\"\n",
    "    #print(fe)\n",
    "    #prevjobs_table = table.scan(TableName=dynamo_appliedjobs_table_name,FilterExpression=fe,ProjectionExpression='id')\n",
    "    #previously_applied_uuids = [item['id']['S'] for item in response['Items']]\n",
    "    response = table.scan(\n",
    "        FilterExpression='userID = :uid',\n",
    "        ExpressionAttributeValues={\n",
    "            ':uid': applicant_uuid\n",
    "        }\n",
    "    )\n",
    "    items=response['Items']\n",
    "    previously_applied_uuids = [item['id'] for item in items]\n",
    "    #best_jobs_json, best_jobs_scores = applicant_embedder.best_jobs(jobs_embedded_json_list, applicant_embedded_json_list[0], jobs_relevant_fields, applicant_relevant_fields, vector_trunc_list,uid_name_jobs, uid_name_applicant,vector_comparison_threshold_list,full_vector_comparison_threshold)\n",
    "    best_jobs_json, best_jobs_scores = applicant_embedder.best_jobs(previously_applied_uuids, jobs_embedded_json_list, applicant_embedded_json_list[applicant_ind], jobs_relevant_fields, applicant_relevant_fields, vector_trunc_list,uid_name_jobs, uid_name_applicant,vector_comparison_threshold_list,full_vector_comparison_threshold)\n",
    "\n",
    "    #get the top few jobs for this applicant\n",
    "    top_n_idx = np.flip(np.argsort(best_jobs_scores)[-num_top_jobs:])\n",
    "    top_n_scores = [best_jobs_scores[i] for i in top_n_idx]\n",
    "    #top_n_jobs = [jobs_data_raw[i] for i in top_n_idx]\n",
    "    #top_n_jobs = [jobs_data_json[i] for i in top_n_idx]\n",
    "    top_n_jobs_json = [best_jobs_json[i] for i in top_n_idx]\n",
    "    #top_n_jobs_keepcols = ['uuid','title','locations','timeKind','payParsed'] \n",
    "    top_n_jobs_keepcols = ['uuid','title','cheeki_score','locations'] \n",
    "\n",
    "    #for i in range(len(top_n_idx)):\n",
    "    #    #top_n_jobs[i]['cheeki_score'] = top_n_jobs_json[i]['cheeki_score']\n",
    "    #    #top_n_jobs[i]['cheeki_score'] = top_n_jobs_json[i]['cheeki_score']\n",
    "\n",
    "    applicant_record = applicant_records[applicant_ind]\n",
    "    #applicant_record['recommended_daily_jobs'] = [i for i in top_n_jobs]\n",
    "    #applicant_record['recommended_daily_jobs'] = [i for i in top_n_jobs_json]\n",
    "    #applicant_record['recommended_daily_jobs'] = [i[top_n_jobs_keepcols] for i in top_n_jobs_json]\n",
    "    applicant_record['recommended_daily_jobs'] = [{key:listelem.get(key) for key in top_n_jobs_keepcols} for listelem in top_n_jobs_json]\n",
    "    job_recommendataions_final.append(applicant_record)\n",
    "\n",
    "#also dump this to a recommended-jobs file:\n",
    "if use_local_scraped_file:\n",
    "    json_object = json.dumps(job_recommendataions_final, indent=4, cls=DecimalEncoder)\n",
    "    with open(\"daily_job_recommendations.json\", \"w\") as outfile:\n",
    "        outfile.write(json_object)\n",
    "else:\n",
    "    # json_string = json.dumps(job_recommendataions_final, cls=DecimalEncoder)\n",
    "    # response = s3.put_object(\n",
    "    #     Body=json_string,\n",
    "    #     Bucket=s3_scraped_bucket_name,\n",
    "    #     Key=s3_scraped_bucket_outfilename\n",
    "    # )\n",
    "    #write the local file too, just for examination:\n",
    "    json_object = json.dumps(job_recommendataions_final, indent=4, cls=DecimalEncoder)\n",
    "    with open(\"daily_job_recommendations.json\", \"w\") as outfile:\n",
    "        outfile.write(json_object)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#job_recommendataions_final\n",
    "\n",
    "#jobs_data_raw[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applicant_records[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #also dump this to a recommended-jobs file:\n",
    "# if use_local_scraped_file:\n",
    "#     json_object = json.dumps(job_recommendataions_final, indent=4, cls=DecimalEncoder)\n",
    "#     with open(\"daily_job_recommendations.json\", \"w\") as outfile:\n",
    "#         outfile.write(json_object)\n",
    "# else:\n",
    "#     # json_string = json.dumps(job_recommendataions_final, cls=DecimalEncoder)\n",
    "#     # response = s3.put_object(\n",
    "#     #     Body=json_string,\n",
    "#     #     Bucket=s3_scraped_bucket_name,\n",
    "#     #     Key=s3_scraped_bucket_outfilename\n",
    "#     # )\n",
    "#     #write the local file too, just for examination:\n",
    "#     json_object = json.dumps(job_recommendataions_final, indent=4, cls=DecimalEncoder)\n",
    "#     with open(\"daily_job_recommendations.json\", \"w\") as outfile:\n",
    "#         outfile.write(json_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applicant_embedded_json_list\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra output set if we want it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#also dump this to a recommended-jobs file:\n",
    "if use_local_scraped_file:\n",
    "    json_object = json.dumps(job_recommendataions_final, indent=4, cls=DecimalEncoder)\n",
    "    with open(\"daily_job_recommendations.json\", \"w\") as outfile:\n",
    "        outfile.write(json_object)\n",
    "else:\n",
    "    json_string = json.dumps(job_recommendataions_final, cls=DecimalEncoder)\n",
    "    response = s3.put_object(\n",
    "        Body=json_string,\n",
    "        Bucket=s3_scraped_bucket_name,\n",
    "        Key=s3_scraped_bucket_outfilename\n",
    "    )\n",
    "    #write the local file too, just for examination:\n",
    "    json_object = json.dumps(job_recommendataions_final, indent=4, cls=DecimalEncoder)\n",
    "    with open(\"daily_job_recommendations.json\", \"w\") as outfile:\n",
    "        outfile.write(json_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_recommendataions_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # and we just want the top few:\n",
    "# top_n_idx = np.flip(np.argsort(best_jobs_scores)[-num_top_jobs:])\n",
    "# top_n_scores = [best_jobs_scores[i] for i in top_n_idx]\n",
    "# top_n_jobs = [jobs_data_raw[i] for i in top_n_idx]\n",
    "\n",
    "# print(applicant_records[0])\n",
    "\n",
    "# [i.items() for i in top_n_jobs]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some quick Visual Confirmations - Does this work as well as we want?\n",
    " - Show the applicant's resume\n",
    " - Show the top 3 jobs we picked for them and their text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_few_jobs_json = [x for _, x in sorted(zip(best_jobs_scores, best_jobs_json),reverse=True)][0:num_top_jobs]\n",
    "# top_few_uuids = [i[uid_name_jobs] for i in top_few_jobs_json]\n",
    "# top_few_job_descriptions = [[i['fullJobDescription'] for i in jobs_data_raw if i[uid_name_jobs]==top_uuid][0] for top_uuid in top_few_uuids]\n",
    "\n",
    "\n",
    "# print('APPLICANT RESUME FULLTEXT:')\n",
    "# print(' ')\n",
    "# print(applicant_record['fullText'])\n",
    "# print(' ')\n",
    "# for i in range(num_top_jobs):\n",
    "#     print('SELECTED JOB NUMBER ', str(int(i)), ':')\n",
    "#     print(' ')\n",
    "#     print(top_few_job_descriptions[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#top_few_job_descriptions\n",
    "#num_top_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
